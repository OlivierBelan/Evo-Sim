[Runner_Info]
runner_type = Reinforcement


[NEURO_EVOLUTION]
verbose = True
# maximize, minimize or closest_to_zero
optimization_type = maximize
algo_name = NES

[Record]
criteria=best_episode_raw_score mean_episode_raw_score
sorted_by=best_episode_raw_score

[Genome_NN]
inputs = 27
hiddens = 64x64
hiddens_active = 128
outputs = 8
inputs_multiplicator = 1
hiddens_multiplicator = 1
outputs_multiplicator = 1

is_self_neuron_connection = True
# Hidden layer can be used as a feedback to another hidden layer, 
# e.g H1->H2 will considered as a feedback and forward connection
is_inter_hidden_feedback = False
is_layer_normalization = False

architecture = I->H1, H1->H2, H2->O
; architecture = I->I, I->H1, I->O, H1->I, H1->H1, H1->O, O->I, O->H1, O->O
; architecture = I->H1, I->H2, I->O, I->I, H1->O, H1->H1, H1->H2, H1->I, H2->O, H2->H1, H2->H2, H2->I, O->O, O->I, O->H1, O->H2
network_type = ANN


[JAX]
pop_size = 128
verbose = False


[bias_neuron_parameter]
mutate_rate = 0.8
max = 10.0
min = -10.0

mu = 0.0
mu_max = 10.0
mu_min = 0.0

sigma = 1.0
sigma_max = 10.0
sigma_min = 0.0
sigma_decay = 0.999


[weight_synapse_parameter]
mutate_rate = 0.8
max = 10.0
min = -10.0

mu = 0.5
mu_max = 10.0
mu_min = -10.0

sigma = 1.0
sigma_max = 10.0
sigma_min = 0.0
sigma_decay = 0.999