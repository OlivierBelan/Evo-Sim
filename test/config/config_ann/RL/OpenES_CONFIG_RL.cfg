[Runner_Info]
runner_type = Reinforcement


[NEURO_EVOLUTION]
verbose = True
# maximize, minimize or closest_to_zero
optimization_type = maximize
algo_name = OpenES

[Record]
sorted_by=best_episode_raw_score
criteria=best_episode_raw_score mean_episode_raw_score

[Genome_NN]
inputs = 2
hiddens = 32x32
hiddens_active = 64
outputs = 1
inputs_multiplicator = 1
hiddens_multiplicator = 1
outputs_multiplicator = 1

is_self_neuron_connection = True
# Hidden layer can be used as a feedback to another hidden layer, 
# e.g H1->H2 will considered as a feedback and forward connection
is_inter_hidden_feedback = False
is_layer_normalization = False

architecture = I->H1, H1->H2, H2->O
# architecture = I->H1, I->O, H1->H1, H1->O, O->H1, O->O
# architecture = I->I, I->H1, I->O, H1->I, H1->H1, H1->O, O->I, O->H1, O->O
# architecture = I->H1, I->H2, I->O, I->I, H1->O, H1->H1, H1->H2, H1->I, H2->O, H2->H1, H2->H2, H2->I, O->O, O->I, O->H1, O->H2
network_type = ANN


[OpenES]
verbose = False
pop_size = 64

# 1. mu params
mu_init=0.0
# 2. Sigma params
sigma_init=0.2
sigma_decay=1.0
sigma_limit=0.01 
# 3. Learning rate params
learning_rate=0.03
learning_rate_decay= 1.0
learning_rate_limit= 0.0001 
# 4. Other params
# whether to use antithetic sampling
antithetic=False            
parameters_decay=0.00           
rank_fitness=False
forget_best=False     
# Adam is the best  
optimizer_name = Adam



; [bias_neuron_parameter]
; max = 10.0
; min = -10.0

; mu = 0.0
; mu_max = 10.0
; mu_min = 0.0

; sigma = 3.0
; sigma_max = 10.0
; sigma_min = 0.0
; sigma_decay = 0.999


[weight_synapse_parameter]
max = 100.0
min = -100.0

mu = 0.5
mu_max = 100.0
mu_min = -100.0

sigma = 1.0
sigma_max = 10.0
sigma_min = 0.0
sigma_decay = 0.999
