[Runner_Info]
runner_type = Reinforcement


[NEURO_EVOLUTION]
verbose = True
# maximize, minimize or closest_to_zero
optimization_type = maximize
algo_name = ARS-evosax

[Record]
criteria=best_episode_raw_score mean_episode_raw_score
sorted_by=best_episode_raw_score

[Genome_NN]
inputs = 27
hiddens = H1:64, H2:64
outputs = 8
inputs_multiplicator = 1
hiddens_multiplicator = 1
outputs_multiplicator = 1

is_self_neuron_connection = True

forward = I->(H1_relu_norm), H1->(H2_relu_norm), H2->O_tanh
# forward = I->(H1_relu_norm), H1->(H2_relu_norm), H2->(H3_relu_norm), H3->O_tanh
# forward = I->(H1_relu,O_tanh), H1->(H1_relu,O_tanh), O->(H1_relu, O_tanh)
# forward = (H1, H2_prev)_tanh_norm->(H1_norm_sigmoid, H2_relu), (H1,H2)_norm_relu->O, O_tanh
# forward = (H1, H2_prev)_tanh_norm->H1, (H1,H2)_norm_relu->O, O_tanh

architecture = I->H1, H1->H2, H2->O
# architecture = I->H1, H1->H2, H2->H3, H3->O
# architecture = I->H1, I->O, H1->H1, H1->O, O->H1, O->O
# architecture = I->I, I->H1, I->O, H1->I, H1->H1, H1->O, O->I, O->H1, O->O
network_type = ANN


[ARS-evosax]
pop_size = 64
verbose = False

elite_ratio = 0.1
optimizer = adam

learning_rate = 0.05
learning_rate_decay = 1.0
learning_rate_limit = 0.001

sigma_init = 0.03
sigma_decay = 1.0
sigma_limit = 0.01

mean_decay = 0.0


[weight_synapse_parameter]
max = 0.0
min = 0.0

mu = 0.0
mu_max = 0.0
mu_min = 0.0

sigma = 0.0
sigma_max = 0.0
sigma_min = 0.0
sigma_decay = 1.0

# [bias_neuron_parameter]
# max = 0.0
# min = 0.0

# mu = 0.0
# mu_max = 0.0
# mu_min = 0.0

# sigma = 0.0
# sigma_max = 0.0
# sigma_min = 0.0
# sigma_decay = 1.0
